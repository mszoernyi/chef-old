etl.destination.path=<%= node[:camus][:etl][:destination] %>
etl.execution.base.path=<%= node[:camus][:etl][:base_path] %>
etl.execution.history.path=<%= node[:camus][:etl][:history_path] %>

camus.message.decoder.class=<%= node[:camus][:decoder] %>
etl.record.writer.provider.class=<%= node[:camus][:writer] %>
mapred.map.tasks=30

# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=6
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=10
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=50

kafka.client.name=camus
# reading from cluster <%= node[:camus][:cluster] %>
kafka.brokers=<%= kafka_connect(node[:camus][:cluster]) %>
kafka.timeout.value=60000

# if whitelist has values, only whitelisted topic are pulled. if empty, everything is pulled
kafka.whitelist.topics=<%= node[:camus][:topics] %>

#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=10

#Controls the submitting of counts to Kafka
post.tracking.counts.to.kafka=false

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=false

etl.default.timezone=UTC
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

kafka.client.buffer.size=8388608
kafka.client.so.timeout=60000
